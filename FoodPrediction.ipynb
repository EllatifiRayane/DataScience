{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Food classification!\n## Polytech' Nice Data Science course 2019\n\n 95% score on the leaderboard By Ellatifi Rayane & Husseini Hussein \n\n\n\nThe food image classification challenge is centered around classifying images of 11 different kinds of food. Some example images from the dataset as well as a breakdown of the classes, complete with example foods, can be seen in the figure and table below. \n\n\n![Food classification](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3423010%2Fd905717ba90ebea44ba2ce87ef4ccaac%2F2019-10-15-155805_1271x1147_scrot.png?generation=1571147905376326&alt=media)\n"},{"metadata":{},"cell_type":"markdown","source":"# Importation & Packages "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#PyTorch, of course\nimport torch\nimport torch.nn as nn\nimport torchvision\n#We will need torchvision transforms for data augmentation\nfrom torchvision import transforms\n\n### utilities\n# tool to print a nice summary of a network, similary to keras' summary\n\n# library to do bash-like wildcard expansion\nimport glob\n\n# others\nimport numpy as np\nimport random\nfrom PIL import Image\nfrom IPython.display import display\nfrom tqdm import tqdm_notebook\n\n\n# a little helper function do directly display a Tensor\ndef display_tensor(t):\n  trans = transforms.ToPILImage()\n  display(trans(t))\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import display\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create the food-11 dataset Class\nCreate the dataset class to load the data. We add a parameter model to control whether its for traning, validation or testing. In the class Food11Dataset, we also implement some data augmentations.\n\nThe transformation for training as data augmentation process: \n\nRandomResizedCrop which crop the given img to 224X224 (since the model we use take this size of images) and aspect ratio.\nRandomHorizontalFlip which horizontally flip the given image randomly with a given probability and RandomVerticalFlip which vertically flip the given image randomly with a given probability.\nNormalize which normalize a tensor image with mean and standard deviation.\n\nThe transformation for valdiation and testing:\n\nResize which crop the given img to 224X224 (since the model we use take this size of images) and aspect ratio.\nNormalize which normalize a tensor image with mean and standard deviation."},{"metadata":{},"cell_type":"markdown","source":"# Training class "},{"metadata":{"trusted":true},"cell_type":"code","source":"class FoodDataset_training_class(torch.utils.data.Dataset):\n  \n  def __init__(self, img_dir):\n    \n    super().__init__()\n    \n    # store directory names\n    self.img_dir = img_dir\n\n    \n    # use glob to get all image names\n    self.img_names = [x.split(\"/\")[6] for x in glob.glob(img_dir + \"/*\")]\n    \n    #set label for each image\n    self.labels = [a.split(\"_\")[0] for a in self.img_names ]\n    \n    # PyTorch transforms\n    self.transform = transforms.Compose([transforms.Resize((224,224)),transforms.RandomHorizontalFlip(p=0.6),transforms.RandomVerticalFlip(p=0.5)\n,transforms.RandomRotation(10)\n                                        ,transforms.ToTensor(),\n                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n\n                                                                                                                \n  \n  def __len__(self):\n    return len(self.img_names)\n    \n  def __getitem__(self,i):\n    return self._read_img(i),int(self.labels[i])\n  \n  def _read_img(self, i):\n    img =Image.open(self.img_dir + \"/\" + self.img_names[i])\n    seed = random.randint(0,2**32)\n    random.seed(seed)\n    \n    return self.transform(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation Class \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class FoodDataset_eval_class(torch.utils.data.Dataset):\n  \n  def __init__(self, img_dir):\n    \n    super().__init__()\n    \n    # store directory names\n    self.img_dir = img_dir\n\n    \n    # use glob to get all image names\n    self.img_names = [x.split(\"/\")[6] for x in glob.glob(img_dir + \"/*\")]\n    \n    #set label for each image\n    self.labels = [a.split(\"_\")[0] for a in self.img_names ]\n    \n    ## We make different transforms when evaluating  \n    # PyTorch transforms\n    self.transform = transforms.Compose([transforms.Resize((224,224))\n                                         \n                                         ,transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n                                        \n\n                                                                                                                \n  \n  def __len__(self):\n    return len(self.img_names)\n    \n  def __getitem__(self,i):\n    return self._read_img(i),int(self.labels[i])\n  \n  def _read_img(self, i):\n    img =Image.open(self.img_dir + \"/\" + self.img_names[i])\n    seed = random.randint(0,2**32)\n    random.seed(seed)\n    \n    return self.transform(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing Class "},{"metadata":{"trusted":true},"cell_type":"code","source":"class FoodDataset_test_class(torch.utils.data.Dataset):\n  \n  def __init__(self, img_dir):\n    \n    super().__init__()\n    \n    # store directory names\n    self.img_dir = img_dir\n\n    \n    # use glob to get all image names\n    self.img_names = [x.split(\"/\")[6] for x in glob.glob(img_dir + \"/*\")]\n    \n    self.subm_names =[a.split(\".\")[0] for a in self.img_names ]\n    \n    ## We make different transforms when evaluating  \n    # PyTorch transforms\n    self.transform = transforms.Compose([transforms.Resize((224,224)) ,transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n                                        \n\n                                                                                                                \n  \n  def __len__(self):\n    return len(self.img_names)\n    \n  def __getitem__(self,i):\n    return self._read_img(i) , int(self.subm_names[i])\n  \n  def _read_img(self, i):\n    img =Image.open(self.img_dir + \"/\" + self.img_names[i])\n\n    \n    return self.transform(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data = FoodDataset_training_class(\"/kaggle/input/polytech-ds-2019/polytech-ds-2019/training\")\ntest_data = FoodDataset_test_class(\"/kaggle/input/polytech-ds-2019/polytech-ds-2019/kaggle_evaluation\")\nvalidation_data = FoodDataset_eval_class(\"/kaggle/input/polytech-ds-2019/polytech-ds-2019/validation\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"food, label = training_data[1100]\ndisplay_tensor(food)\nlabel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\nNUM_WORKERS = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_dl = torch.utils.data.DataLoader(training_data, batch_size = BATCH_SIZE , shuffle= True , num_workers= NUM_WORKERS)\nvalidation_dl = torch.utils.data.DataLoader(validation_data,batch_size=BATCH_SIZE,shuffle=True ,num_workers=NUM_WORKERS)\ntest_dl = torch.utils.data.DataLoader(test_data,batch_size=BATCH_SIZE,num_workers=NUM_WORKERS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transforms = tta.Compose(\n    [\n        tta.HorizontalFlip(),\n        tta.Rotate90(angles=[0, 180]),\n        tta.Scale(scales=[1, 2, 4]),\n        tta.Multiply(factors=[0.9, 1, 1.1]),        \n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The resnet101 model is trained on Imagenet database, so it get 1000 output for the classifier(fully connected layer). In our food11 dataset it only have 11 categories, so we must modify the classifier of this model so the output vector would be 11 instead of 1000."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = torchvision.models.wide_resnet101_2(pretrained=True,progress=True)\nprint(model)\nmodel.fc=nn.Sequential(nn.Linear(2048,1024),nn.ReLU(),nn.Dropout2d(p=0.5),nn.Linear(1024,11),nn.LogSoftmax(dim=1))\n#tta_model = tta.ClassificationTTAWrapper(model, transforms,merge_mode='mean')\n#print(model)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training part "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Training the model\nLEARNING_RATE = 0.001\nModel =model.cuda()\ncriterion =nn.NLLLoss()\noptimizer =torch.optim.SGD( Model.parameters(),lr=LEARNING_RATE,momentum=0.9,weight_decay=0.004)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_EPOCHS = 9\nepoch_loss, epoch_acc, epoch_val_loss, epoch_val_acc = [], [],[], []\n\nfor e in range(NB_EPOCHS):\n    print(\"Epoch :\", e)\n    running_loss = 0\n    running_accuracy = 0\n    \n    Model.train()\n    \n    for i ,batch in enumerate(training_dl):\n        x = batch[0]\n        Labels = batch[1]\n        \n        x= x.cuda()\n        Labels = Labels.cuda()\n        \n        y = Model(x)\n        loss =criterion(y,Labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        with torch.no_grad():\n            running_loss +=loss.item()\n            running_accuracy += (y.max(1)[1] == Labels).sum().item()\n    print(\"Training accuracy: \" , running_accuracy/float(len(training_data)) ,\n         \"Training loss: \", running_loss/len(training_data))\n    epoch_loss.append(running_loss/len(training_data) )\n    epoch_acc.append(running_accuracy/len(training_data))\n    \n    \n    Model.eval()\n    \n    running_val_loss = 0\n    running_val_accuracy = 0\n    \n    for i ,batch in enumerate(validation_dl):\n        with torch.no_grad():\n            x = batch[0]\n            Labels = batch[1]\n        \n            x= x.cuda()\n            Labels = Labels.cuda()\n        \n            y = Model(x)\n            loss =criterion(y,Labels)\n\n        \n        \n            running_val_loss +=loss.item()\n            running_val_accuracy += (y.max(1)[1] == Labels).sum().item()\n    print(\"validation accuracy: \" , running_val_accuracy/float(len(validation_data)) ,\n         \"validation loss: \", running_val_loss/len(validation_data))\n    epoch_val_loss.append(running_val_loss/len(validation_data) )\n    epoch_val_acc.append(running_val_accuracy/len(validation_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.arange(NB_EPOCHS)\nplt.figure()\nplt.plot(x, epoch_acc, x, epoch_val_acc)\n\nplt.figure()\nplt.plot(x, epoch_loss, x, epoch_val_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\nimage_names = []\n\nModel.eval()\n\nfor i ,batch in enumerate(test_dl):\n    with torch.no_grad():\n        x = batch[0]\n        Id = batch[1]\n        \n        x= x.cuda()\n        \n        \n        y = Model(x)\n        \n    predictions.append(y.max(1)[1])\n    image_names.append(Id)\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names =[]\nfor i in image_names :\n    names = names + (list(i.numpy()))\nlen(names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict=[]\nfor i in predictions :\n    predict = predict + (list(i.cpu().numpy()))\nlen(predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_tup = list(zip(names,predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission part \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(list_of_tup,columns=['Id','Category'])\nsubmission\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"goldenboys.csv\",index=False ,encoding ='utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}